Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5l6.pt to yolov5l6.pt...
freezing model.0.conv.conv.weight
freezing model.0.conv.bn.weight
freezing model.0.conv.bn.bias
freezing model.1.conv.weight
freezing model.1.bn.weight
freezing model.1.bn.bias
freezing model.2.cv1.conv.weight
freezing model.2.cv1.bn.weight
freezing model.2.cv1.bn.bias
freezing model.2.cv2.conv.weight
freezing model.2.cv2.bn.weight
freezing model.2.cv2.bn.bias
freezing model.2.cv3.conv.weight
freezing model.2.cv3.bn.weight
freezing model.2.cv3.bn.bias
freezing model.2.m.0.cv1.conv.weight
freezing model.2.m.0.cv1.bn.weight
freezing model.2.m.0.cv1.bn.bias
freezing model.2.m.0.cv2.conv.weight
freezing model.2.m.0.cv2.bn.weight
freezing model.2.m.0.cv2.bn.bias
freezing model.2.m.1.cv1.conv.weight
freezing model.2.m.1.cv1.bn.weight
freezing model.2.m.1.cv1.bn.bias
freezing model.2.m.1.cv2.conv.weight
freezing model.2.m.1.cv2.bn.weight
freezing model.2.m.1.cv2.bn.bias
freezing model.2.m.2.cv1.conv.weight
freezing model.2.m.2.cv1.bn.weight
freezing model.2.m.2.cv1.bn.bias
freezing model.2.m.2.cv2.conv.weight
freezing model.2.m.2.cv2.bn.weight
freezing model.2.m.2.cv2.bn.bias
freezing model.3.conv.weight
freezing model.3.bn.weight
freezing model.3.bn.bias
freezing model.4.cv1.conv.weight
freezing model.4.cv1.bn.weight
freezing model.4.cv1.bn.bias
freezing model.4.cv2.conv.weight
freezing model.4.cv2.bn.weight
freezing model.4.cv2.bn.bias
freezing model.4.cv3.conv.weight
freezing model.4.cv3.bn.weight
freezing model.4.cv3.bn.bias
freezing model.4.m.0.cv1.conv.weight
freezing model.4.m.0.cv1.bn.weight
freezing model.4.m.0.cv1.bn.bias
freezing model.4.m.0.cv2.conv.weight
freezing model.4.m.0.cv2.bn.weight
freezing model.4.m.0.cv2.bn.bias
freezing model.4.m.1.cv1.conv.weight
freezing model.4.m.1.cv1.bn.weight
freezing model.4.m.1.cv1.bn.bias
freezing model.4.m.1.cv2.conv.weight
freezing model.4.m.1.cv2.bn.weight
freezing model.4.m.1.cv2.bn.bias
freezing model.4.m.2.cv1.conv.weight
freezing model.4.m.2.cv1.bn.weight
freezing model.4.m.2.cv1.bn.bias
freezing model.4.m.2.cv2.conv.weight
freezing model.4.m.2.cv2.bn.weight
freezing model.4.m.2.cv2.bn.bias
freezing model.4.m.3.cv1.conv.weight
freezing model.4.m.3.cv1.bn.weight
freezing model.4.m.3.cv1.bn.bias
freezing model.4.m.3.cv2.conv.weight
freezing model.4.m.3.cv2.bn.weight
freezing model.4.m.3.cv2.bn.bias
freezing model.4.m.4.cv1.conv.weight
freezing model.4.m.4.cv1.bn.weight
freezing model.4.m.4.cv1.bn.bias
freezing model.4.m.4.cv2.conv.weight
freezing model.4.m.4.cv2.bn.weight
freezing model.4.m.4.cv2.bn.bias
freezing model.4.m.5.cv1.conv.weight
freezing model.4.m.5.cv1.bn.weight
freezing model.4.m.5.cv1.bn.bias
freezing model.4.m.5.cv2.conv.weight
freezing model.4.m.5.cv2.bn.weight
freezing model.4.m.5.cv2.bn.bias
freezing model.4.m.6.cv1.conv.weight
freezing model.4.m.6.cv1.bn.weight
freezing model.4.m.6.cv1.bn.bias
freezing model.4.m.6.cv2.conv.weight
freezing model.4.m.6.cv2.bn.weight
freezing model.4.m.6.cv2.bn.bias
freezing model.4.m.7.cv1.conv.weight
freezing model.4.m.7.cv1.bn.weight
freezing model.4.m.7.cv1.bn.bias
freezing model.4.m.7.cv2.conv.weight
freezing model.4.m.7.cv2.bn.weight
freezing model.4.m.7.cv2.bn.bias
freezing model.4.m.8.cv1.conv.weight
freezing model.4.m.8.cv1.bn.weight
freezing model.4.m.8.cv1.bn.bias
freezing model.4.m.8.cv2.conv.weight
freezing model.4.m.8.cv2.bn.weight
freezing model.4.m.8.cv2.bn.bias
freezing model.5.conv.weight
freezing model.5.bn.weight
freezing model.5.bn.bias
freezing model.6.cv1.conv.weight
freezing model.6.cv1.bn.weight
freezing model.6.cv1.bn.bias
freezing model.6.cv2.conv.weight
freezing model.6.cv2.bn.weight
freezing model.6.cv2.bn.bias
freezing model.6.cv3.conv.weight
freezing model.6.cv3.bn.weight
freezing model.6.cv3.bn.bias
freezing model.6.m.0.cv1.conv.weight
freezing model.6.m.0.cv1.bn.weight
freezing model.6.m.0.cv1.bn.bias
freezing model.6.m.0.cv2.conv.weight
freezing model.6.m.0.cv2.bn.weight
freezing model.6.m.0.cv2.bn.bias
freezing model.6.m.1.cv1.conv.weight
freezing model.6.m.1.cv1.bn.weight
freezing model.6.m.1.cv1.bn.bias
freezing model.6.m.1.cv2.conv.weight
freezing model.6.m.1.cv2.bn.weight
freezing model.6.m.1.cv2.bn.bias
freezing model.6.m.2.cv1.conv.weight
freezing model.6.m.2.cv1.bn.weight
freezing model.6.m.2.cv1.bn.bias
freezing model.6.m.2.cv2.conv.weight
freezing model.6.m.2.cv2.bn.weight
freezing model.6.m.2.cv2.bn.bias
freezing model.6.m.3.cv1.conv.weight
freezing model.6.m.3.cv1.bn.weight
freezing model.6.m.3.cv1.bn.bias
freezing model.6.m.3.cv2.conv.weight
freezing model.6.m.3.cv2.bn.weight
freezing model.6.m.3.cv2.bn.bias
freezing model.6.m.4.cv1.conv.weight
freezing model.6.m.4.cv1.bn.weight
freezing model.6.m.4.cv1.bn.bias
freezing model.6.m.4.cv2.conv.weight
freezing model.6.m.4.cv2.bn.weight
freezing model.6.m.4.cv2.bn.bias
freezing model.6.m.5.cv1.conv.weight
freezing model.6.m.5.cv1.bn.weight
freezing model.6.m.5.cv1.bn.bias
freezing model.6.m.5.cv2.conv.weight
freezing model.6.m.5.cv2.bn.weight
freezing model.6.m.5.cv2.bn.bias
freezing model.6.m.6.cv1.conv.weight
freezing model.6.m.6.cv1.bn.weight
freezing model.6.m.6.cv1.bn.bias
freezing model.6.m.6.cv2.conv.weight
freezing model.6.m.6.cv2.bn.weight
freezing model.6.m.6.cv2.bn.bias
freezing model.6.m.7.cv1.conv.weight
freezing model.6.m.7.cv1.bn.weight
freezing model.6.m.7.cv1.bn.bias
freezing model.6.m.7.cv2.conv.weight
freezing model.6.m.7.cv2.bn.weight
freezing model.6.m.7.cv2.bn.bias
freezing model.6.m.8.cv1.conv.weight
freezing model.6.m.8.cv1.bn.weight
freezing model.6.m.8.cv1.bn.bias
freezing model.6.m.8.cv2.conv.weight
freezing model.6.m.8.cv2.bn.weight
freezing model.6.m.8.cv2.bn.bias
freezing model.7.conv.weight
freezing model.7.bn.weight
freezing model.7.bn.bias
freezing model.8.cv1.conv.weight
freezing model.8.cv1.bn.weight
freezing model.8.cv1.bn.bias
freezing model.8.cv2.conv.weight
freezing model.8.cv2.bn.weight
freezing model.8.cv2.bn.bias
freezing model.8.cv3.conv.weight
freezing model.8.cv3.bn.weight
freezing model.8.cv3.bn.bias
freezing model.8.m.0.cv1.conv.weight
freezing model.8.m.0.cv1.bn.weight
freezing model.8.m.0.cv1.bn.bias
freezing model.8.m.0.cv2.conv.weight
freezing model.8.m.0.cv2.bn.weight
freezing model.8.m.0.cv2.bn.bias
freezing model.8.m.1.cv1.conv.weight
freezing model.8.m.1.cv1.bn.weight
freezing model.8.m.1.cv1.bn.bias
freezing model.8.m.1.cv2.conv.weight
freezing model.8.m.1.cv2.bn.weight
freezing model.8.m.1.cv2.bn.bias
freezing model.8.m.2.cv1.conv.weight
freezing model.8.m.2.cv1.bn.weight
freezing model.8.m.2.cv1.bn.bias
freezing model.8.m.2.cv2.conv.weight
freezing model.8.m.2.cv2.bn.weight
freezing model.8.m.2.cv2.bn.bias
freezing model.9.conv.weight
freezing model.9.bn.weight
freezing model.9.bn.bias
Plotting labels...
[34m[1mautoanchor: [39m[22mAnalyzing anchors... anchors/target = 3.98, Best Possible Recall (BPR) = 1.0000
                 all       3987      10507      0.911       0.88      0.941      0.551
                 all       3987      10507      0.976      0.961      0.988      0.657
                 all       3987      10507      0.982      0.967      0.989      0.681
                 all       3987      10507      0.988      0.978      0.994      0.743
                 all       3987      10507      0.988      0.984      0.995       0.78
                 all       3987      10507      0.987      0.986      0.996      0.799
                 all       3987      10507      0.987      0.989      0.995      0.809
                 all       3987      10507       0.99      0.991      0.996      0.827
                 all       3987      10507      0.993       0.99      0.996      0.838
                 all       3987      10507      0.994      0.989      0.996      0.849
                 all       3987      10507      0.994      0.993      0.996      0.859
                 all       3987      10507       0.99      0.997      0.996      0.865
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148M/148M [00:03<00:00, 50.2MB/s]
Overriding model.yaml nc=80 with nc=3
                 from  n    params  module                                  arguments
  0                -1  1      7040  models.common.Focus                     [3, 64, 3]
  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
  2                -1  3    156928  models.common.C3                        [128, 128, 3]
  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]
  4                -1  9   1611264  models.common.C3                        [256, 256, 9]
  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]
  6                -1  9   6433792  models.common.C3                        [512, 512, 9]
  7                -1  1   3540480  models.common.Conv                      [512, 768, 3, 2]
  8                -1  3   5611008  models.common.C3                        [768, 768, 3]
  9                -1  1   7079936  models.common.Conv                      [768, 1024, 3, 2]
 10                -1  1   2624512  models.common.SPP                       [1024, 1024, [3, 5, 7]]
 11                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]
 12                -1  1    787968  models.common.Conv                      [1024, 768, 1, 1]
 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 14           [-1, 8]  1         0  models.common.Concat                    [1]
 15                -1  3   6200832  models.common.C3                        [1536, 768, 3, False]
 16                -1  1    394240  models.common.Conv                      [768, 512, 1, 1]
 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 18           [-1, 6]  1         0  models.common.Concat                    [1]
 19                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]
 20                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 22           [-1, 4]  1         0  models.common.Concat                    [1]
 23                -1  3    690688  models.common.C3                        [512, 256, 3, False]
 24                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]
 25          [-1, 20]  1         0  models.common.Concat                    [1]
 26                -1  3   2495488  models.common.C3                        [512, 512, 3, False]
 27                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]
 28          [-1, 16]  1         0  models.common.Concat                    [1]
 29                -1  3   5807616  models.common.C3                        [1024, 768, 3, False]
 30                -1  1   5309952  models.common.Conv                      [768, 768, 3, 2]
 31          [-1, 12]  1         0  models.common.Concat                    [1]
 32                -1  3  10496000  models.common.C3                        [1536, 1024, 3, False]
 33  [23, 26, 29, 32]  1     61536  models.yolo.Detect                      [3, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [256, 512, 768, 1024]]
Model Summary: 638 layers, 76670944 parameters, 76670944 gradients, 116.5 GFLOPs
Transferred 824/832 items from yolov5l6.pt
Scaled weight_decay = 0.0005
[34m[1moptimizer:[39m[22m SGD with parameter groups 137 weight, 141 weight (no decay), 141 bias
[34m[1mtrain: [39m[22mScanning 'C:\Users\balaji\Desktop\Traffic_Camera_Tracking\Main_Code\Yolo_data\labels\train.cache' images and labels... 22598 found,
[34m[1mval: [39m[22mScanning 'C:\Users\balaji\Desktop\Traffic_Camera_Tracking\Main_Code\Yolo_data\labels\valid.cache' images and labels... 3987 found, 0
Image sizes 1280 train, 1280 val
Using 8 dataloader workers
Logging results to runs\train\exp2
Starting training for 30 epochs...
     Epoch   gpu_mem       box       obj       cls    labels  img_size
      0/29     6.41G   0.03092   0.01585   0.01318        35      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [27:49<00:00,  1.69it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:22<00:00,  1.75it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
      1/29     6.42G   0.02317  0.008342  0.003243        30      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [27:01<00:00,  1.74it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:21<00:00,  1.76it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
      2/29     6.46G   0.02048  0.007618  0.002226        34      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [26:50<00:00,  1.75it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:26<00:00,  1.71it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
      3/29     6.46G   0.01681  0.007005  0.001502        29      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [26:47<00:00,  1.76it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:21<00:00,  1.77it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
      4/29     6.46G   0.01463  0.006422    0.0011        28      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [26:47<00:00,  1.76it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:21<00:00,  1.77it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
      5/29     6.46G   0.01335   0.00602 0.0008486        18      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [26:47<00:00,  1.76it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:26<00:00,  1.71it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
      6/29     6.46G   0.01244  0.005804 0.0008069        33      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [26:47<00:00,  1.76it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:21<00:00,  1.77it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
      7/29     6.46G   0.01154  0.005527 0.0006519        29      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [26:47<00:00,  1.76it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:21<00:00,  1.77it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
      8/29     6.46G   0.01086  0.005321 0.0005565        33      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [26:47<00:00,  1.76it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:25<00:00,  1.71it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
      9/29     6.46G   0.01063  0.005191 0.0005447        37      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [26:47<00:00,  1.76it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:21<00:00,  1.77it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
     10/29     6.46G   0.01004  0.004974 0.0004592        32      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [26:47<00:00,  1.76it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:21<00:00,  1.77it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
     11/29     6.46G  0.009598  0.004906 0.0004541        25      1280: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2825/2825 [26:47<00:00,  1.76it/s]
               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [02:25<00:00,  1.71it/s]
     Epoch   gpu_mem       box       obj       cls    labels  img_size
     12/29     6.46G  0.009391  0.004801 0.0005185        45      1280:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2443/2825 [23:16<03:38,  1.75it/s]
Traceback (most recent call last):
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\Main_Code\Traffic_Camera_Tracking\YoloV5\yolo_v5_main_files\train.py", line 602, in <module>
    main(opt)
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\Main_Code\Traffic_Camera_Tracking\YoloV5\yolo_v5_main_files\train.py", line 500, in main
    train(opt.hyp, opt, device)
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\Main_Code\Traffic_Camera_Tracking\YoloV5\yolo_v5_main_files\train.py", line 317, in train
    pred = model(imgs)  # forward
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\lib\site-packages\torch\nn\modules\module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\Main_Code\Traffic_Camera_Tracking\YoloV5\yolo_v5_main_files\models\yolo.py", line 124, in forward
    return self.forward_once(x, profile, visualize)  # single-scale inference, train
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\Main_Code\Traffic_Camera_Tracking\YoloV5\yolo_v5_main_files\models\yolo.py", line 155, in forward_once
    x = m(x)  # run
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\lib\site-packages\torch\nn\modules\module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\Main_Code\Traffic_Camera_Tracking\YoloV5\yolo_v5_main_files\models\common.py", line 136, in forward
    return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\lib\site-packages\torch\nn\modules\module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\Main_Code\Traffic_Camera_Tracking\YoloV5\yolo_v5_main_files\models\common.py", line 44, in forward
    return self.act(self.bn(self.conv(x)))
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\lib\site-packages\torch\nn\modules\module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\lib\site-packages\torch\nn\modules\conv.py", line 443, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\balaji\Desktop\Traffic_Camera_Tracking\lib\site-packages\torch\nn\modules\conv.py", line 439, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
KeyboardInterrupt